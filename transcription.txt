[Document(page_content="GWENDOLYN STRIPLING: Hello. And welcome to Introduction\nto Generative AI. My name is Dr.\nGwendolyn Stripling. And I am the\nartificial intelligence technical curriculum developer\nhere at Google Cloud. In this course, you learn\nto define generative AI, explain how generative AI works,\ndescribe generative AI model types, and describe\ngenerative AI applications. Generative AI is a type\nof artificial intelligence technology that can produce\nvarious types of content, including text, imagery,\naudio, and synthetic data. But what is artificial\nintelligence? Well, since we are\ngoing to explore generative artificial\nintelligence, let's provide a bit of context. So two very common\nquestions asked are what is artificial\nintelligence and what is the difference\nbetween AI and machine learning. One way to think about it\nis that AI is a discipline, like physics for example. AI is a branch of\ncomputer science that deals with the creation\nof intelligence agents, which are systems that can reason,\nand learn, and act autonomously. Essentially, AI has to do\nwith the theory and methods to build machines that\nthink and act like humans. In this discipline, we\nhave machine learning, which is a subfield of AI. It is a program or system that\ntrains a model from input data. That trained model can\nmake useful predictions from new or never\nbefore seen data drawn from the same one\nused to train the model. Machine learning\ngives the computer the ability to learn without\nexplicit programming. Two of the most common classes\nof machine learning models are unsupervised and\nsupervised ML models. The key difference\nbetween the two is that, with supervised\nmodels, we have labels. Labeled data is data that comes\nwith a tag like a name, a type, or a number. Unlabeled data is data\nthat comes with no tag. This graph is an\nexample of the problem that a supervised model\nmight try to solve. For example, let's say you\nare the owner of a restaurant. You have historical\ndata of the bill amount and how much different people\ntipped based on order type and whether it was\npicked up or delivered. In supervised learning, the\nmodel learns from past examples to predict future values,\nin this case tips. So here the model uses\nthe total bill amount to predict the future tip amount\nbased on whether an order was picked up or delivered. This is an example\nof the problem that an unsupervised\nmodel might try to solve. So here you want to look\nat tenure and income and then group or\ncluster employees to see whether someone\nis on the fast track. Unsupervised problems\nare all about discovery, about looking at the raw data\nand seeing if it naturally falls into groups. Let's get a little deeper\nand show this graphically as understanding\nthese concepts are the foundation for your\nunderstanding of generative AI. In supervised learning,\ntesting data values or x are input into the model. The model outputs a prediction\nand compares that prediction to the training data\nused to train the model. If the predicted test data\nvalues and actual training data values are far apart,\nthat's called error. And the model tries\nto reduce this error until the predicted and actual\nvalues are closer together. This is a classic\noptimization problem. Now that we've\nexplored the difference between artificial intelligence\nand machine learning, and supervised and\nunsupervised learning, let's briefly explore\nwhere deep learning fits as a subset of\nmachine learning methods. While machine learning\nis a broad field that encompasses many\ndifferent techniques, deep learning is a type\nof machine learning that uses artificial\nneural networks, allowing them to process more\ncomplex patterns than machine learning. Artificial neural networks are\ninspired by the human brain. They are made up of many\ninterconnected nodes or neurons that can learn to perform tasks\nby processing data and making predictions. Deep learning models\ntypically have many layers of neurons, which\nallows them to learn more complex patterns than\ntraditional machine learning models. And neural networks can use\nboth labeled and unlabeled data. This is called\nsemi-supervised learning. In semi-supervised\nlearning, a neural network is trained on a small\namount of labeled data and a large amount\nof unlabeled data. The labeled data helps\nthe neural network to learn the basic\nconcepts of the task while the unlabeled data\nhelps the neural network to generalize to new examples. Now we finally get to\nwhere generative AI fits into this AI discipline. Gen AI is a subset of\ndeep learning, which means it uses artificial\nneural networks, can process both labeled\nand unlabeled data using supervised, unsupervised,\nand semi-supervised methods. Large language models are also\na subset of deep learning. Deep learning models, or machine\nlearning models in general, can be divided into two types,\ngenerative and discriminative. A discriminative model\nis a type of model that is used to classify or\npredict labels for data points. Discriminative\nmodels are typically trained on a data set\nof labeled data points. And they learn the relationship\nbetween the features of the data points\nand the labels. Once a discriminative\nmodel is trained, it can be used to predict the\nlabel for new data points. A generative model\ngenerates new data instances based on a learned probability\ndistribution of existing data. Thus generative models\ngenerate new content. Take this example here. The discriminative model learns\nthe conditional probability distribution or the\nprobability of y, our output, given x, our\ninput, that this is a dog and classifies it as\na dog and not a cat. The generative model learns the\njoint probability distribution or the probability of\nx and y and predicts the conditional probability\nthat this is a dog and can then generate\na picture of a dog. So to summarize,\ngenerative models can generate new data instances\nwhile discriminative models discriminate between different\nkinds of data instances. The top image shows\na traditional machine learning model which\nattempts to learn the relationship between\nthe data and the label, or what you want to predict. The bottom image shows\na generative AI model which attempts to learn\npatterns on content so that it can generate new content. A good way to distinguish\nwhat is gen AI and what is not is shown in this illustration. It is not gen AI when the\noutput, or y, or label is a number or a class, for\nexample spam or not spam, or a probability. It is gen AI when the output is\nnatural language, like speech or text, an image or\naudio, for example. Visualizing this mathematically\nwould look like this. If you haven't seen\nthis for a while, the y is equal to f of\nx equation calculates the dependent output of a\nprocess given different inputs. The y stands for\nthe model output. The f embodies the function\nused in the calculation. And the x represents the input\nor inputs used for the formula. So the model output is a\nfunction of all the inputs. If the y is the number,\nlike predicted sales, it is not gen AI. If y is a sentence,\nlike define sales, it is generative as the question\nwould elicit a text response. The response would be based\non all the massive large data the model was\nalready trained on. To summarize at a high level,\nthe traditional, classical supervised and unsupervised\nlearning process takes training code and\nlabel data to build a model. Depending on the\nuse case or problem, the model can give\nyou a prediction. It can classify something\nor cluster something. We use this example to show\nyou how much more robust the gen AI process is. The gen AI process can take\ntraining code, label data, and unlabeled data\nof all data types and build a foundation model. The foundation model can\nthen generate new content. For example, text, code,\nimages, audio, video, et cetera. We've come a long away from\ntraditional programming to neural networks\nto generative models. In traditional\nprogramming, we used to have to hard code the rules\nfor distinguishing a cat-- the type, animal; legs,\nfour; ears, two; fur, yes; likes yarn and catnip. In the wave of\nneural networks, we could give the network\npictures of cats and dogs and ask is this a cat and\nit would predict a cat. In the generative\nwave, we as users can generate our own\ncontent, whether it be text, images, audio,\nvideo, et cetera, for example models like PaLM or\nPathways Language Model, or LAMBDA, Language Model\nfor Dialogue Applications, ingest very, very large data\nfrom the multiple sources across the internet and\nbuild foundation language models we can use simply\nby asking a question, whether typing it into\na prompt or verbally talking into the prompt itself. So when you ask it\nwhat's a cat, it can give you everything it\nhas learned about a cat. Now we come to our\nformal definition. What is generative AI? Gen AI is a type of\nartificial intelligence that creates new content\nbased on what it has learned from existing content. The process of learning\nfrom existing content is called training and\nresults in the creation of a statistical model\nwhen given a prompt. AI uses the model to predict\nwhat an expected response might be and this generates\nnew content. Essentially, it learns\nthe underlying structure of the data and\ncan then generate new samples that are similar\nto the data it was trained on. As previously mentioned, a\ngenerative language model can take what it has learned\nfrom the examples it's been shown and create\nsomething entirely new based on that information. Large language models are\none type of generative AI since they generate novel\ncombinations of text in the form of natural\nsounding language. A generative image\nmodel takes an image as input and can output text,\nanother image, or video. For example, under\nthe output text, you can get visual\nquestion answering while under output image, an\nimage completion is generated. And under output video,\nanimation is generated. A generative language\nmodel takes text as input and can output more text, an\nimage, audio, or decisions. For example, under\nthe output text, question answering is generated. And under output image,\na video is generated. We've stated that generative\nlanguage models learn about patterns and language\nthrough training data, then, given some text, they\npredict what comes next. Thus generative language models\nare pattern matching systems. They learn about patterns\nbased on the data you provide. Here is an example. Based on things it's learned\nfrom its training data, it offers predictions of how\nto complete this sentence, I'm making a sandwich with\npeanut butter and jelly. Here is the same\nexample using Bard, which is trained on a\nmassive amount of text data and is able to\ncommunicate and generate humanlike text in response\nto a wide range of prompts and questions. Here is another example. The meaning of life is-- and Bart gives you\na contextual answer and then shows the highest\nprobability response. The power of generative AI comes\nfrom the use of transformers. Transformers produced\na 2018 revolution in natural language processing. At a high level, a\ntransformer model consists of an\nencoder and decoder. The encoder encodes\nthe input sequence and passes it to\nthe decoder, which learns how to decode\nthe representation for a relevant task. In transformers, hallucinations\nare words or phrases that are generated\nby the model that are often nonsensical or\ngrammatically incorrect. Hallucinations can be caused\nby a number of factors, including the model is not\ntrained on enough data, or the model is trained\non noisy or dirty data, or the model is not\ngiven enough context, or the model is not\ngiven enough constraints. Hallucinations can be a\nproblem for transformers because they can make the output\ntext difficult to understand. They can also make\nthe model more likely to generate incorrect\nor misleading information. A prompt is a\nshort piece of text that is given to the large\nlanguage model as input. And it can be used to control\nthe output of the model in a variety of ways. Prompt design is the\nprocess of creating a prompt that will\ngenerate the desired output from a large language model. As previously mentioned,\ngen AI depends a lot on the training data that\nyou have fed into it. And it analyzes the patterns\nand structures of the input data and thus learns. But with access to a browser\nbased prompt, you, the user, can generate your own content. We've shown illustrations of the\ntypes of input based upon data. Here are the\nassociated model types. Text-to-text. Text-to-text models take\na natural language input and produces a text output. These models are trained\nto learn the mapping between a pair of text, e.g. for example, translation\nfrom one language to another. Text-to-image. Text-to-image models are trained\non a large set of images, each captioned with a\nshort text description. Diffusion is one method\nused to achieve this. Text-to-video and text-to-3D. Text-to-video models aim to\ngenerate a video representation from text input. The input text can be anything\nfrom a single sentence to a full script. And the output is a video that\ncorresponds to the input text. Similarly, text-to-3D\nmodels generate three dimensional objects that\ncorrespond to a user's text description. For example, this can be used\nin games or other 3D worlds. Text-to-task. Text-to-task models are trained\nto perform a defined task or action based on text input. This task can be a\nwide range of actions such as answering a question,\nperforming a search, making a prediction, or\ntaking some sort of action. For example, a\ntext-to-task model could be trained to navigate a\nweb UI or make changes to a doc through the GUI. A foundation model is a\nlarge AI model pre-trained on a vast quantity of data\ndesigned to be adapted or fine tuned to a wide range\nof downstream tasks, such as sentiment analysis,\nimage captioning, and object recognition. Foundation models\nhave the potential to revolutionize many\nindustries, including health care, finance,\nand customer service. They can be used to\ndetect fraud and provide personalized customer support. Vertex AI offers a\nmodel garden that includes foundation models. The language foundation\nmodels include PaLM API for chat and text. The vision foundation models\nincludes stable diffusion, which has been shown to\nbe effective at generating high quality images\nfrom text descriptions. Let's say you have\na use case where you need to gather sentiments\nabout how your customers are feeling about your\nproduct or service. You can use the classification\ntask sentiment analysis task model for just that purpose. And what if you needed to\nperform occupancy analytics? There is a task model\nfor your use case. Shown here are gen\nAI applications. Let's look at an example\nof code generation shown in the second block\nunder code at the top. In this example, I've input a\ncode file conversion problem, converting from Python to JSON. I use Bard. And I insert into the\nprompt box the following. I have a Pandas DataFrame with\ntwo columns, one with the file name and one with the hour\nin which it is generated. I'm trying to convert\nthis into a JSON file in the format shown onscreen. Bard returns the steps I need\nto do this and the code snippet. And here my output\nis in a JSON format. It gets better. I happen to be using Google's\nfree, browser-based Jupyter Notebook, known as Colab. And I simply export the\nPython code to Google's Colab. To summarize, Bart\ncode generation can help you debug your\nlines of source code, explain your code\nto you line by line, craft SQL queries\nfor your database, translate code from one\nlanguage to another, and generate documentation\nand tutorials for source code. Generative AI Studio lets you\nquickly explore and customize gen AI models that you can\nleverage in your applications on Google Cloud. Generative AI Studio helps\ndevelopers create and deploy Gen AI models by providing a\nvariety of tools and resources that make it easy\nto get started. For example, there's a\nlibrary of pre-trained models. There is a tool for\nfine tuning models. There is a tool for deploying\nmodels to production. And there is a community\nforum for developers to share ideas and collaborate. Generative AI App\nBuilder lets you create gen AI apps without\nhaving to write any code. Gen AI App Builder has a\ndrag and drop interface that makes it easy to\ndesign and build apps. It has a visual\neditor that makes it easy to create\nand edit app content. It has a built-in\nsearch engine that allows users to search for\ninformation within the app. And it has a\nconversational AI Engine that helps users to\ninteract with the app using natural language. You can create your own digital\nassistants, custom search engines, knowledge bases,\ntraining applications, and much more. PaLM API lets you\ntest and experiment with Google's large language\nmodels and gen AI tools. To make prototyping quick\nand more accessible, developers can integrate\nPaLM API with Maker suite and use it to access the\nAPI using a graphical user interface. The suite includes a number of\ndifferent tools such as a model training tool, a model\ndeployment tool, and a model monitoring tool. The model training tool helps\ndevelopers train ML models on their data using\ndifferent algorithms. The model deployment tool helps\ndevelopers deploy ML models to production with a number of\ndifferent deployment options. The model monitoring\ntool helps developers monitor the performance\nof their ML models in production using a\ndashboard and a number of different metrics. Thank you for watching\nour course, Introduction to Generative AI.", metadata={'source': 'G2fqAlgmoPo', 'title': 'Introduction to Generative AI', 'description': 'Unknown', 'view_count': 1577643, 'thumbnail_url': 'https://i.ytimg.com/vi/G2fqAlgmoPo/hq720.jpg', 'publish_date': '2023-05-08 00:00:00', 'length': 1327, 'author': 'Google Cloud Tech'})]